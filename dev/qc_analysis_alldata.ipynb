{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import gc\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "- pull from data team endpoints\n",
    "- add time as default request\n",
    "- collapse within script\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = ''\n",
    "token = ''\n",
    "array = 'stats/GP_sci_parameters'\n",
    "\n",
    "QC_PARAMETER_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12578/qcparameters/'\n",
    "DEPLOYEMENT_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12587/events/deployment/inv/'\n",
    "DATA_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting qc data...\n"
     ]
    }
   ],
   "source": [
    "print(\"requesting qc data...\")\n",
    "r = requests.get(QC_PARAMETER_URL, auth=(username, token))\n",
    "data = r.json()\n",
    "\n",
    "refdes_qc_list = []\n",
    "parameter_qc_list = []\n",
    "globalrange_min_qc_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data[i]['qcParameterPK']['qcId'] == 'dataqc_globalrangetest_minmax' \\\n",
    "    and data[i]['qcParameterPK']['parameter'] == 'dat_min':\n",
    "        \n",
    "        refdes = data[i]['qcParameterPK']['refDes']['subsite']+'-'+\\\n",
    "            data[i]['qcParameterPK']['refDes']['node']+'-'+\\\n",
    "            data[i]['qcParameterPK']['refDes']['sensor']\n",
    "        refdes_qc_list.append(refdes)\n",
    "        \n",
    "        parameter = data[i]['qcParameterPK']['streamParameter']\n",
    "        parameter_qc_list.append(parameter)\n",
    "        \n",
    "        globalrange_min = data[i]['value']\n",
    "        globalrange_min_qc_list.append(globalrange_min)\n",
    "\n",
    "qc_dict = {\n",
    "    'refdes':refdes_qc_list,\n",
    "    'parameter':parameter_qc_list,\n",
    "    'global_range_min':globalrange_min_qc_list,\n",
    "}     \n",
    "        \n",
    "globalrange_min_qc_data = pd.DataFrame(qc_dict,columns=['refdes','parameter','global_range_min'])\n",
    "\n",
    "refdes_qc_list = []\n",
    "parameter_qc_list = []\n",
    "globalrange_max_qc_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data[i]['qcParameterPK']['qcId'] == 'dataqc_globalrangetest_minmax' \\\n",
    "    and data[i]['qcParameterPK']['parameter'] == 'dat_max':\n",
    "        \n",
    "        refdes = data[i]['qcParameterPK']['refDes']['subsite']+'-'+\\\n",
    "            data[i]['qcParameterPK']['refDes']['node']+'-'+\\\n",
    "            data[i]['qcParameterPK']['refDes']['sensor']\n",
    "        refdes_qc_list.append(refdes)\n",
    "        \n",
    "        parameter = data[i]['qcParameterPK']['streamParameter']\n",
    "        parameter_qc_list.append(parameter)\n",
    "        \n",
    "        globalrange_max = data[i]['value']\n",
    "        globalrange_max_qc_list.append(globalrange_max)\n",
    "\n",
    "qc_dict = {\n",
    "    'refdes':refdes_qc_list,\n",
    "    'parameter':parameter_qc_list,\n",
    "    'global_range_max':globalrange_max_qc_list,\n",
    "}     \n",
    "        \n",
    "globalrange_max_qc_data = pd.DataFrame(qc_dict,columns=['refdes','parameter','global_range_max'])\n",
    "\n",
    "global_ranges = pd.merge(globalrange_min_qc_data,globalrange_max_qc_data, on=['refdes','parameter'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "working on cabled\n",
      "building deployment info requests...\n",
      "sending deployment info requests...\n"
     ]
    }
   ],
   "source": [
    "# set up some functions\n",
    "def request_data(url,username,token):\n",
    "    auth = (username, token)\n",
    "    return session.get(url,auth=auth)\n",
    "\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day\n",
    "\n",
    "def diff_days(d1,d2):\n",
    "    return (d2 - d1).days\n",
    "\n",
    "ntp_epoch = datetime.datetime(1900, 1, 1)\n",
    "unix_epoch = datetime.datetime(1970, 1, 1)\n",
    "ntp_delta = (unix_epoch - ntp_epoch).total_seconds()\n",
    "\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=20)\n",
    "session = requests.session()\n",
    "retry = Retry(\n",
    "        total=10,\n",
    "        backoff_factor=0.3,\n",
    "    )\n",
    "adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100,max_retries=retry,pool_block=True)\n",
    "session.mount('http://', adapter)\n",
    "\n",
    "logging.basicConfig(filename=array+'_requests.log',level=logging.DEBUG)\n",
    "\n",
    "refdes = 'input/' + array + '.csv'\n",
    "refdes_list = pd.read_csv(refdes)\n",
    "refdes_list = refdes_list['refdes']\n",
    "refdes_list = refdes_list.drop_duplicates()\n",
    "\n",
    "print('\\n'+\"working on\", array)\n",
    "print(\"building deployment info requests...\")\n",
    "asset_requests = []\n",
    "for i in refdes_list:\n",
    "    sub_site = i[:8]\n",
    "    platform = i[9:14]\n",
    "    instrument = i[15:27]\n",
    "    asset_url_inputs = '/'.join((sub_site, platform, instrument))\n",
    "    request_url = DEPLOYEMENT_URL+asset_url_inputs+'/-1'\n",
    "    asset_requests.append(request_url)\n",
    "\n",
    "print(\"sending deployment info requests...\")\n",
    "ref_des_list = []\n",
    "start_time_list = []\n",
    "end_time_list = []\n",
    "deployment_list = []\n",
    "\n",
    "future_to_url = {pool.submit(request_data, url, username, token): url for url in asset_requests}\n",
    "for future in concurrent.futures.as_completed(future_to_url):\n",
    "    try:\n",
    "        asset_info = future.result()\n",
    "        asset_info = asset_info.json()\n",
    "\n",
    "        for i in range(len(asset_info)):\n",
    "            refdes = asset_info[i]['referenceDesignator']\n",
    "            ref_des_list.append(refdes)\n",
    "\n",
    "            deployment = asset_info[i]['deploymentNumber']\n",
    "            deployment_list.append(deployment)\n",
    "\n",
    "            start = asset_info[i]['eventStartTime']\n",
    "            end = asset_info[i]['eventStopTime']\n",
    "\n",
    "            try:\n",
    "                start_time = datetime.datetime.utcfromtimestamp(start/1000.0)\n",
    "                start_time_list.append(start_time)\n",
    "\n",
    "                end_time = datetime.datetime.utcfromtimestamp(end/1000.0)\n",
    "                end_time_list.append(end_time)\n",
    "\n",
    "            except:\n",
    "                end_time = datetime.datetime.utcnow()\n",
    "                end_time_list.append(end_time)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "data_dict = {\n",
    "    'refdes':ref_des_list,\n",
    "    'deployment':deployment_list,\n",
    "    'start_time':start_time_list,\n",
    "    'end_time':end_time_list}\n",
    "deployment_data = pd.DataFrame(data_dict, columns = ['refdes', 'deployment','start_time', 'end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating days between deployment dates...\n",
      "building data request urls...\n"
     ]
    }
   ],
   "source": [
    "print(\"calculating days between deployment dates...\")\n",
    "deployment_data_days = pd.DataFrame(columns = ['refdes', 'deployment','date'])\n",
    "\n",
    "# calculate days between deployment dates\n",
    "for index, row in deployment_data.iterrows():\n",
    "    start_time = row['start_time']\n",
    "    end_time = row['end_time']\n",
    "    periods = diff_days(start_time, end_time)\n",
    "    start_time = to_integer(start_time)\n",
    "    total_days = pd.DataFrame({'date' : pd.date_range(str(start_time),periods=periods,freq='D')})\n",
    "\n",
    "    total_days['refdes'] = row['refdes']\n",
    "    total_days['deployment'] = row['deployment']\n",
    "    deployment_data_days = deployment_data_days.append(total_days)\n",
    "\n",
    "# re-order data frame columns\n",
    "deployment_data_days = deployment_data_days[['refdes', 'deployment','date']]\n",
    "\n",
    "print(\"building data request urls...\")\n",
    "deployment_data_days['start_date'] = deployment_data_days['date'] + datetime.timedelta(seconds=5)\n",
    "deployment_data_days['end_date'] = deployment_data_days['date'] + datetime.timedelta(seconds=86395)\n",
    "\n",
    "qc_db_input = 'input/' + array + '.csv'\n",
    "qc_db_input = pd.read_csv(qc_db_input)\n",
    "refdes_streams_df = qc_db_input[['refdes','method','stream','parameter']]\n",
    "refdes_streams_df = refdes_streams_df.drop_duplicates()\n",
    "\n",
    "request_inputs = pd.merge(refdes_streams_df,deployment_data_days, on='refdes')\n",
    "\n",
    "request_inputs['subsite'] = request_inputs.refdes.str[:8]\n",
    "request_inputs['platform'] = request_inputs.refdes.str[9:14]\n",
    "request_inputs['instrument'] = request_inputs.refdes.str[15:27]\n",
    "request_inputs['start_date'] = pd.to_datetime(request_inputs['start_date'])\n",
    "request_inputs['start_date'] = request_inputs.start_date.dt.strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
    "request_inputs['end_date'] = pd.to_datetime(request_inputs['end_date'])\n",
    "request_inputs['end_date'] = request_inputs.end_date.dt.strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
    "\n",
    "request_inputs['urls'] = DATA_URL+\\\n",
    "                        request_inputs.subsite+\\\n",
    "                        '/'+request_inputs.platform+\\\n",
    "                        '/'+request_inputs.instrument+\\\n",
    "                        '/'+request_inputs.method+\\\n",
    "                        '/'+request_inputs.stream+\\\n",
    "                        '?beginDT='+request_inputs.start_date+\\\n",
    "                        '&endDT='+request_inputs.end_date+\\\n",
    "                        '&limit=50'\n",
    "\n",
    "request_urls = request_inputs['urls'].drop_duplicates()\n",
    "request_urls = request_urls.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending data requests for cabled...\n",
      "\t current time: 2017-11-29 00:14:30.287136\n",
      "\t 66579 data requests being sent\n",
      "\t check cabled_requests.log file in your working directory for progress\n"
     ]
    }
   ],
   "source": [
    "print(\"sending data requests for\", array+'...')\n",
    "print('\\t',\"current time:\", datetime.datetime.now())\n",
    "print('\\t',len(request_urls ),\"data requests being sent\")\n",
    "print('\\t',\"check\",array+\"_requests.log\",\"file in your working directory for progress\")\n",
    "\n",
    "finaldf = pd.DataFrame()\n",
    "missing = []\n",
    "\n",
    "future_to_url = {pool.submit(request_data, url, username, token): url for url in request_urls}\n",
    "for future in concurrent.futures.as_completed(future_to_url):\n",
    "#     url = future_to_url[future]\n",
    "    try:\n",
    "        data = future.result() \n",
    "        data = data.json()\n",
    "\n",
    "        refdes_list = []\n",
    "        parameter_list = []\n",
    "        timestamp_list = []\n",
    "        value_list = []\n",
    "        \n",
    "        # use this to speed up the loop\n",
    "#         df = pd.DataFrame.from_records(map(json.loads, map(json.dumps,data)))\n",
    "        \n",
    "        # iterate through data points to extract time stamps\n",
    "        for i in range(len(data)):\n",
    "            timestamp = data[i]['time']\n",
    "            timestamp = datetime.datetime.utcfromtimestamp(timestamp - ntp_delta).replace(microsecond=0)\n",
    "            timestamp = timestamp.date()\n",
    "      \n",
    "            # get refdes from the response and create data frame y with the corresponding gloabl range values\n",
    "            refdes = data[i]['pk']['subsite'] + '-' + data[i]['pk']['node'] + '-' + data[i]['pk']['sensor']\n",
    "            x = global_ranges['refdes'] == refdes\n",
    "            y = global_ranges[x]\n",
    "\n",
    "            # check if global range list contains an entry for the refdes\n",
    "            templist = list(global_ranges['refdes'])\n",
    "            if refdes not in templist:\n",
    "                missing.append(refdes)\n",
    "                \n",
    "                \n",
    "            # iterate through all variables in global range data frame y, then iterate through keys in data point\n",
    "            # to find matching keys, then grab values\n",
    "            for var in y.parameter.values:\n",
    "                for j in data[i].keys():\n",
    "                    if var == j:\n",
    "                        z = data[i][j]\n",
    "                        \n",
    "                        # conditional to handle 2d datasets, in which case the first non nan value is checked\n",
    "                        if type(z) != list:\n",
    "                            refdes_list.append(refdes)\n",
    "                            parameter_list.append(var)\n",
    "                            value_list.append(z)\n",
    "                            timestamp_list.append(timestamp)\n",
    "                        else:\n",
    "                            u = next(u for u in z if not isnan(u))\n",
    "                            refdes_list.append(refdes)\n",
    "                            parameter_list.append(var)\n",
    "                            value_list.append(u)\n",
    "                            timestamp_list.append(timestamp)\n",
    "                            \n",
    "\n",
    "        # create data frame from lists collected above\n",
    "        data_dict = {\n",
    "            'refdes':refdes_list,\n",
    "            'parameter':parameter_list,\n",
    "            'value':value_list,\n",
    "            'date':timestamp_list}\n",
    "        response_data = pd.DataFrame(data_dict, columns = ['refdes','parameter','value','date'])\n",
    "\n",
    "        # merge into data frame with global range values and check if value between global ranges\n",
    "        df = y.merge(response_data,how='outer')\n",
    "        df['pass'] = (df['value'] < pd.to_numeric(df['global_range_max'])) & \\\n",
    "                        (df['value'] > pd.to_numeric(df['global_range_min']))\n",
    "\n",
    "        # collapse the data frame to calculate percent of data points that pass the test for that day\n",
    "        df2 = df['pass'].groupby([df['refdes'], \\\n",
    "                    df['parameter'],\\\n",
    "                  df['date'] \\\n",
    "                   ]).sum().reset_index()\n",
    "        df2['percent'] = (df2['pass'] / len(data)) * 100\n",
    "        df2['data_points'] = len(data)\n",
    "        df2 = df2[['refdes','parameter','date','data_points','percent']]\n",
    "\n",
    "        # append result for this ref des and day to final data frame\n",
    "        finaldf = finaldf.append(df2)\n",
    "            \n",
    "    except:\n",
    "#         print('no data for ', url)\n",
    "        pass\n",
    "\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          refdes  \\\n",
      "145  RS01SBPD-DP01A-03-FLCDRA102   \n",
      "146  RS01SBPD-DP01A-04-FLNTUA102   \n",
      "147  RS01SBPD-DP01A-04-FLNTUA102   \n",
      "148  RS01SBPD-DP01A-04-FLNTUA102   \n",
      "149  RS01SBPD-DP01A-05-OPTAAC102   \n",
      "150  RS01SBPD-DP01A-05-OPTAAC102   \n",
      "151  RS01SBPS-PC01A-06-VADCPA101   \n",
      "152  RS01SBPS-PC01A-07-CAMDSC102   \n",
      "153  RS01SBPS-PC01A-4A-DOSTAD103   \n",
      "154  RS01SBPS-PC01A-4A-DOSTAD103   \n",
      "155  RS01SBPS-PC01A-4A-DOSTAD103   \n",
      "156  RS01SBPS-PC01A-4A-DOSTAD103   \n",
      "157  RS01SBPS-PC01A-4A-DOSTAD103   \n",
      "158  RS01SBPS-PC01A-4A-DOSTAD103   \n",
      "159  RS01SBPS-PC01A-4A-DOSTAD103   \n",
      "160  RS01SBPS-PC01A-4B-PHSENA102   \n",
      "161  RS01SBPS-PC01A-4C-FLORDD103   \n",
      "162  RS01SBPS-SF01A-2A-DOFSTA102   \n",
      "163  RS01SBPS-SF01A-2A-DOFSTA102   \n",
      "164  RS01SBPS-SF01A-2A-DOFSTA102   \n",
      "165  RS01SBPS-SF01A-2A-DOFSTA102   \n",
      "166  RS01SBPS-SF01A-2A-DOFSTA102   \n",
      "167  RS01SBPS-SF01A-2A-DOFSTA102   \n",
      "168  RS01SBPS-SF01A-2D-PHSENA101   \n",
      "169  RS01SBPS-SF01A-3A-FLORTD101   \n",
      "170  RS01SBPS-SF01A-3D-SPKIRA101   \n",
      "171  RS01SBPS-SF01A-4A-NUTNRA101   \n",
      "172  RS01SBPS-SF01A-4A-NUTNRA101   \n",
      "173  RS01SBPS-SF01A-4B-VELPTD102   \n",
      "174  RS01SBPS-SF01A-4B-VELPTD102   \n",
      "..                           ...   \n",
      "341  RS03INT1-MJ03C-07-RASFLA301   \n",
      "342  RS03INT1-MJ03C-07-RASFLA301   \n",
      "343  RS03INT1-MJ03C-09-THSPHA301   \n",
      "344  RS03INT1-MJ03C-09-THSPHA301   \n",
      "345  RS03INT1-MJ03C-09-THSPHA301   \n",
      "346  RS03INT1-MJ03C-09-THSPHA301   \n",
      "347  RS03INT1-MJ03C-09-THSPHA301   \n",
      "348  RS03INT1-MJ03C-09-THSPHA301   \n",
      "349  RS03INT1-MJ03C-09-THSPHA301   \n",
      "350  RS03INT1-MJ03C-09-THSPHA301   \n",
      "351  RS03INT1-MJ03C-10-TRHPHA301   \n",
      "352  RS03INT1-MJ03C-10-TRHPHA301   \n",
      "353  RS03INT1-MJ03C-10-TRHPHA301   \n",
      "354  RS03INT1-MJ03C-10-TRHPHA301   \n",
      "355  RS03INT2-MJ03D-05-OBSSPA305   \n",
      "356  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "357  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "358  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "359  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "360  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "361  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "362  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "363  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "364  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "365  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "366  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "367  RS03INT2-MJ03D-06-BOTPTA303   \n",
      "368  RS03INT2-MJ03D-12-VEL3DB304   \n",
      "369  RS03INT2-MJ03D-12-VEL3DB304   \n",
      "370  RS03INT2-MJ03D-12-VEL3DB304   \n",
      "\n",
      "                                             parameter  \n",
      "145                  flcdr_x_mmp_cds_fluorometric_cdom  \n",
      "146  flntu_x_mmp_cds_total_volume_scattering_coeffi...  \n",
      "147         flntu_x_mmp_cds_fluorometric_chlorophyll_a  \n",
      "148                        flntu_x_mmp_cds_bback_total  \n",
      "149                                   beam_attenuation  \n",
      "150                                 optical_absorption  \n",
      "151                            error_seawater_velocity  \n",
      "152                                                NaN  \n",
      "153                                            density  \n",
      "154                                 practical_salinity  \n",
      "155                                   dissolved_oxygen  \n",
      "156                               seawater_temperature  \n",
      "157                                  seawater_pressure  \n",
      "158                              seawater_conductivity  \n",
      "159                                      ctd_tc_oxygen  \n",
      "160                       phsen_thermistor_temperature  \n",
      "161                    seawater_scattering_coefficient  \n",
      "162                                            density  \n",
      "163                                 practical_salinity  \n",
      "164                               seawater_temperature  \n",
      "165                                  seawater_pressure  \n",
      "166                              seawater_conductivity  \n",
      "167                         corrected_dissolved_oxygen  \n",
      "168                       phsen_thermistor_temperature  \n",
      "169                    seawater_scattering_coefficient  \n",
      "170                           spkir_downwelling_vector  \n",
      "171                              nitrate_concentration  \n",
      "172                         salinity_corrected_nitrate  \n",
      "173                          velpt_d_eastward_velocity  \n",
      "174                         velpt_d_northward_velocity  \n",
      "..                                                 ...  \n",
      "341                                               temp  \n",
      "342                                                NaN  \n",
      "343                                      thsph_temp_th  \n",
      "344                                      thsph_temp_tl  \n",
      "345                                           thsph_ph  \n",
      "346                                       thsph_ph_acl  \n",
      "347                                     thsph_ph_noref  \n",
      "348                                 thsph_ph_noref_acl  \n",
      "349                                      thsph_sulfide  \n",
      "350                                     thsph_hydrogen  \n",
      "351                              vent_fluid_temperaure  \n",
      "352                           vent_fluid_chloride_conc  \n",
      "353                                     vent_fluid_orp  \n",
      "354                              trhph_thermistor_temp  \n",
      "355                                                NaN  \n",
      "356                        corrected_compass_direction  \n",
      "357                            seafloor_tilt_magnitude  \n",
      "358                            seafloor_tilt_direction  \n",
      "359                                    bottom_pressure  \n",
      "360                                   botsflu_meanpres  \n",
      "361                                  botsflu_meandepth  \n",
      "362                                   botsflu_5minrate  \n",
      "363                                  botsflu_10minrate  \n",
      "364                                   botsflu_predtide  \n",
      "365                                   botsflu_daydepth  \n",
      "366                                    botsflu_4wkrate  \n",
      "367                                    botsflu_8wkrate  \n",
      "368                        eastward_turbulent_velocity  \n",
      "369                       northward_turbulent_velocity  \n",
      "370                          upward_turbulent_velocity  \n",
      "\n",
      "[226 rows x 2 columns] \n",
      "\n",
      "the following reference designators are missing global range values\n",
      " set()\n"
     ]
    }
   ],
   "source": [
    "finaldf.to_csv('output/'+array+'_quality.csv', index=False)\n",
    "\n",
    "# get list of instruments and parameters for which no data was returned, or no qc values are available\n",
    "returns = finaldf[['refdes','parameter']].drop_duplicates()\n",
    "expected = qc_db_input[['refdes','parameter']].drop_duplicates()\n",
    "not_found = returns.merge(expected,indicator=True, how='outer')\n",
    "not_found = not_found[not_found['_merge'] == 'right_only']\n",
    "del not_found['_merge']\n",
    "print(not_found,'\\n')\n",
    "\n",
    "# get list of just instruments missing qc values all together\n",
    "print('the following reference designators are missing global range values\\n',set(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "request_inputs = request_inputs[['refdes','method','stream','parameter','date']]\n",
    "request_inputs['date'] = pd.to_datetime(request_inputs['date'])\n",
    "finaldf['date'] = pd.to_datetime(finaldf['date'])\n",
    "# finaldf['date'] = finaldf['time_stamp']\n",
    "finaldf = finaldf[['refdes','parameter','date','data_points','percent']]\n",
    "new = request_inputs.merge(finaldf, on=(['refdes','parameter','date']),how='outer')\n",
    "new.to_csv('output/'+array+'_examine.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "stats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
